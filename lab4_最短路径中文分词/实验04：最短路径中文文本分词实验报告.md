# 实验 04 最短路径中文文本分词
罗毅凡 2024202715

## 1.需求分析

1. 输入一段文本，对该文本进行分词，并将分词结果输出。

2. 设计交互界面或演示程序。

3. 拓展功能：实现N最短路径分词。

## 2. 概要设计

* 为实现上述中文文本分词功能：首先，需要读取词表构建哈希表存储词典；然后，对输入文本处理构建图结构；最后，使用Dijkstra算法或动态规划算法计算出从句首到句尾的最短路径，得到最短路径中文文本分词结果并输出结果。

### 1. 哈希表的数据类型为：
```c
typedef struct WordNode {
    char word[MAX_WORD_LEN];
    int freq;
    char pos[MAX_POS_LEN];
    struct WordNode *next;
} WordNode;

typedef struct {
    WordNode* buckets[TABLE_SIZE];
    int total_count;
    long total_freq;
} WordDict;
```

### 2. 图的数据类型为：
```c
typedef struct Graph{
    double **adj;
    int nodes_num;
    char *source;
}Graph;
```

### 3. 本程序包含四个模块

#### （a）主程序模块：设计命令行交互，接收中文文本，输出分词结果

#### （b）哈希表词典构建模块：读取词表构建哈希表存储词典

#### （c）文本图结构构建模块：对输入文本处理构建图结构

#### （d）最短路径查找分词模块：使用Dijkstra算法或动态规划算法计算出从句首到句尾的最短路径

## 3. 详细设计

### 1. 哈希表词典构建

将词表中的词及其频率依次读取插入到哈希表中，这里的哈希函数使用djb2哈希算法，并使用链地址法处理哈希冲突

* djb2哈希算法：

```c
unsigned long hash_func(const char *str) {
    unsigned long hash = 5381;
    int c;
    while ((c = *str++))
        hash = ((hash << 5) + hash) + c;
    return hash % TABLE_SIZE;
}
```

### 2. 文本图结构构建

* 实现逻辑：

  1. 图的节点数为输入中文文本字数+1，包含开始和结尾字间隔

  2. 遍历输入中文文本的所有子串：若词典中包含子串，则该子串起始位置和终点位置节点含有一条边，边的权重为 $log(\frac{\text{词典总词频}}{\text{该词词频}})$

这样我们就完成了文本图结构的构建

* 代码：
```c
Graph* build_graph(char* s, WordDict* dict) {
    int len = strlen(s);
    Graph* G = (Graph*)malloc(sizeof(Graph));
    G->nodes_num = len + 1;
    G->source = strdup(s);

    G->adj = (double**)malloc(sizeof(double*) * G->nodes_num);
    for (int i = 0; i < G->nodes_num; i++) {
        G->adj[i] = (double*)malloc(sizeof(double) * G->nodes_num);
        for (int j = 0; j < G->nodes_num; j++) {
            G->adj[i][j] = INFINITY_WEIGHT;
        }
    }

    char temp_word[MAX_WORD_LEN];
    long total_freq = dict->total_freq > 0 ? dict->total_freq : 1;

    for (int i = 0; i < len; i++) {
        int char_len = get_utf8_len(s[i]);
        if (i + char_len <= len) {G->adj[i][i + char_len] = 20.0; }
        for (int j = i + 1; j <= len && (j - i) < MAX_WORD_LEN; j++) {
            int sub_len = j - i;
            if (sub_len < char_len) continue;
            strncpy(temp_word, s + i, sub_len);
            temp_word[sub_len] = '\0';
            int freq = get_word_freq(dict, temp_word);
            if (freq > 0) {
                G->adj[i][j] = log((double)total_freq) - log((double)freq);
            } 
        }
    } 
    return G;
}
```

### 3. 最短路径查找分词

* 使用Dijkstra算法实现最短路径的查找，伪代码如下：
```
Algorithm Dijkstra_Segmentation(Graph G, int N):
    // 1. 初始化
    Create PriorityQueue Q  // 优先队列，存储 (cost, node_index)
    Array dist[0...N]       // 存储到每个节点的最小代价
    Array parent[0...N]     // 记录路径来源
    
    Initialize all dist[] = INFINITY
    Initialize all parent[] = -1
    
    dist[0] = 0
    Q.push(0, 0)  // 将起点推入队列

    // 2. 主循环
    While Q is not empty:
        // 贪心策略：取出当前累积代价最小的节点 u
        u = Q.pop_min_node()
        
        // 如果已经处理过该节点的最短路，跳过 (可选优化)
        If u is visited: continue
        Mark u as visited

        // 松弛操作 (Relax)：遍历 u 的所有邻居 v
        // 在分词图中，邻居 v 就是 u 后面的节点 (如 u+1, u+2...)
        For each neighbor v of u in G:
            weight = G.adj[u][v]  // 边的权重 (-log P)
            
            // 如果发现了一条更短的路
            If dist[u] + weight < dist[v]:
                dist[v] = dist[u] + weight
                parent[v] = u
                Q.push(v, dist[v])  // 更新队列

    // 3. 回溯 (Backtrack) 得到分词结果
    path = []
    curr = N
    While curr != 0:
        prev = parent[curr]
        Add segment (prev -> curr) to path
        curr = prev
    Reverse path
    Return path
```

* 使用动态规划能够实现查找top-N的最短路径，伪代码如下：
```
Algorithm N_Shortest_Path_DP(Graph G, int N, int K):
    // dp[i] 是一个列表，存储到达 i 的 Top-K 个 (cost, parent_node, parent_rank)
    Array dp[0...N] of List
    
    dp[0].add({cost: 0, parent: -1, rank: -1})

    // 1. 线性扫描
    For i from 0 to N-1:
        
        // 遍历 i 的每一个后继节点 j
        For each neighbor j of i:
            weight = G.adj[i][j]
            
            // 关键：节点 i 有 K 条路径，每一条都可以延续到 j
            For rank_k from 0 to dp[i].size() - 1:
                prev_path = dp[i][rank_k]
                
                new_cost = prev_path.cost + weight
                
                // 将新路径加入 j 的候选池
                dp[j].add_candidate({
                    cost: new_cost,
                    parent: i,
                    rank: rank_k  // 记住我是接在 i 的第几名后面的
                })

        // 2. 在处理完所有指向 j 的边后，对 j 的候选池排序并截断
        Sort dp[j] by cost ascending
        Keep only top K elements in dp[j]

    // 3. 回溯
    // 从 dp[N] 中取前 K 个结果分别回溯
    Return Backtrack(dp, N, K)
```

## 使用手册

### 1. 概述 & 核心功能

本系统是一个基于 N-最短路径 (N-Shortest Path) 算法的高效中文分词工具。它通过构建有向无环图 (DAG)，利用动态规划思想寻找句子中概率最高的 N 种切分方式。

### 2. 命令行交互操作

程序提供一个交互式的命令行界面，用户可以实时输入待分词中文文本实现中文文本最短路径分词。

## 功能测试报告

### 1. 测试概述

本次测试旨在验证 N-最短路径 (NSP) 分词算法在处理长句、成语、口语俗语以及具有歧义的中文文本时的准确性与鲁棒性；观察 Top-N (N=3) 结果在解决分词歧义上的作用。

### 2. 综合测试记录

|ID|测试文本片段|Top-1 分词结果 (最优解)|结果判定|备注|
|-|-|-|-|-|
|T01|曾经有一份真诚的爱情...|曾经/有/一份/真诚/的/爱情...|Pass|"长句切分准确|""一万年""识别为整体"|
|T02|说好了一辈子，少一年...|说好/了/一辈子，少/一年...|Pass|"准确识别""一辈子""、""一个/时辰"""|
|T03|以前我没得选...|以前/我/没/得/选...|Pass|"口语""没得选""切分合理"|
|T04|站着把钱挣了！|站/着/把/钱/挣/了/！|Pass|"动词""站着""被切分为单字(视词典而定)"|
|T05|做人如果没梦想...|做人/如果/没/梦想...|Pass|"准确识别""做人""、""咸鱼""等词汇"|

### 3.详细测试案例分析

#### 案例 T01：《大话西游》经典台词

* 输入：...如果非要给这份爱加上一个期限，我希望是：一万年

* 分析：

Result 1 (最优)：...加上/一个/期限/，/我/希望/是/：/一万年。

系统成功识别了“一万年”作为一个固定的时间词组，而不是切分为“一/万年”（Result 2）。这说明在词典中“一万年”的联合概率高于单独切分的概率。

标点处理：输入中的省略号 ...... 被切分为 ./././././.。这是因为词典中没有收录六个点的省略号，系统触发了未登录词处理机制，将其逐个字节（ASCII字符）切分，逻辑符合预期。

歧义处理：Result 1 选择了 非/要，而 Result 3 选择了 非要。这表明在该词典统计中，单字“非”和“要”的高频组合优于双字词“非要”。

#### 案例 T02：《霸王别姬》台词

* 输入：说好了一辈子，少一年、一个月、一天、一个时辰，都不算一辈子！

* 分析：

量词处理：系统完美处理了多种时间结构。

一辈子：识别正确。

一年、一个月、一天：识别正确。

一个/时辰：Result 1 将其分开，这通常是因为“一个”作为高频量词修饰语，独立成词的概率极高，切分结果符合现代汉语习惯。

歧义消除：对比 Result 1 (说好/了) 和 Result 2 (说/好/了)，算法倾向于将“说好”视为一个动词短语，这在语义上更贴合原句语境。

#### 案例 T03：《无间道》台词

* 输入：以前我没得选，现在我想做个好人

* 分析：

Result 1：没/得/选。

Result 3：没得/选。

这里展示了 N-最短路径的威力。在口语中，“没得”对应“没有”，但在标准语料中频率可能较低。Result 3 保留了这种可能性。

好人 被正确识别为合成词，而非 好/人。

#### 案例 T04：《让子弹飞》台词

* 输入：站着把钱挣了！

* 分析：

Result 1：站/着/把/钱/挣/了/！

这是一个典型的动词连续使用场景。虽然“站着”在很多词典里是一个词，但由于“站”和“着”作为单字频率极高（尤其是“着”作为助词），在单纯的最短路径算法下，往往倾向于切分。这也反映了基于词频的统计分词的一个特点：倾向于切分出高频单字。

#### 案例 T05：《少林足球》台词

* 输入：做人如果没梦想，跟咸鱼有什么区别。

分析：

常用搭配：做人、梦想、区别 均被正确识别。

### 测试结论与评价

本次测试表明，实现的 N-最短路径分词系统功能完备，性能良好：

1. 准确性：在 Top-1 结果中，绝大多数常用词、成语和时间短语都能被正确切分。

2. 鲁棒性：能够处理含有标点符号、英文（ASCII）和未登录词的句子，未出现程序崩溃或乱码现象（除了预期的单字节切分）。

3. 多结果召回：Top-3 列表成功捕获了细微的切分歧义（如“非/要” vs “非要”，“没/得” vs “没得”），为后续的语义分析提供了丰富候选。

改进建议:标点符号优化：建议在预处理阶段或词典中加入 ……、—— 等特殊标点符号，避免被切分为单个字符。